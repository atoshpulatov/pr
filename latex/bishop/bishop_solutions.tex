% Notes and solutions


% -------------------------------------------------
% Package imports
% -------------------------------------------------
\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}% Input encoding
\usepackage[english]{babel}% Set language to english
\usepackage{graphicx}% For importing graphics
\usepackage{amsthm, amsfonts, amssymb, bm}% All the AMS packages
\usepackage{mathtools}% Fixes a few AMS bugs
\usepackage[expansion=false]{microtype}% Fixes to make typography better
\usepackage{hyperref}% For \href{URL}{text}
\usepackage{fancyhdr}% For fancy headers
\usepackage[sharp]{easylist}% Easy nested lists
\usepackage{parskip}% Web-like paragraphs
\usepackage{multicol}% For multiple columns
\usepackage{tikz-cd}% For diagrams
\usepackage{microtype}
\usepackage{listings}% To include source-code
\usepackage[margin = 2.5cm, includehead]{geometry}% May be used to set margins
\usepackage{nicefrac}% Enables \nicefrac{nom}{denom}
%\usepackage[sc]{mathpazo}% A nice font, alternative to CM
\usepackage{booktabs}
\usepackage{fancyvrb} % fancy verbatim

% -------------------------------------------------
% Package setup
% -------------------------------------------------

\newcommand{\Title}{Solutions to\\``Pattern Recognition and Machine Learning''\\by Bishop}
%\newcommand{\Subtitle}{A summary of main ideas and concepts}
\newcommand{\Author}{\texttt{tommyod} @ github}
\newcommand{\listSpace}{-0.5em}% Global list space

\title{\Title}
\author{\Author}

% Shortcuts for sets and other stuff in mathematics
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Reg}{\mathcal{R}}
\newcommand{\Class}{\mathcal{C}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\sf}[1]{\mathsf{#1}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}

% Shortcuts for probability distributions
\DeclareMathOperator{\N}{\mathcal{N}}
\DeclareMathOperator{\Bern}{\operatorname{Bern}}
\DeclareMathOperator{\Beta}{\operatorname{Beta}}
\DeclareMathOperator{\Bin}{\operatorname{Bin}}
\DeclareMathOperator{\St}{\operatorname{St}}
\DeclareMathOperator{\U}{\operatorname{U}}
\DeclareMathOperator{\KL}{\operatorname{KL}}

% Shortcuts for statistical operators
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\var}{\operatorname{var}}
\DeclareMathOperator{\cov}{\operatorname{cov}}
\DeclareMathOperator{\SD}{\operatorname{SD}}
\renewcommand{\H}{\operatorname{H}}



% Change the section command to inluce lines, save space, etc.
%\usepackage{titlesec}
%\titlespacing\subsection{0pt}{6pt plus 4pt minus 2pt}{-0.8em plus 4pt minus 2pt}
%\titlespacing\paragraph{0pt}{6pt plus 4pt minus 2pt}{8pt plus 4pt minus 2pt}
%\titleformat{\section}
%{\normalfont\Large\bfseries}{\thesection}{0em}{$\vartriangleright$ }[{\titlerule[1.2pt]}]

% -------------------------------------------------
% Document start
% -------------------------------------------------
\begin{document}
	
\maketitle
\begin{abstract}
This document contains some notes and solutions to the book TODO.fødselsnrfødselsnr

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.25\linewidth]{figs/bishop}
	\caption{The front cover of \cite{bishop_pattern_2011}.}
	\label{fig:duda}
\end{figure}
\end{abstract}

\clearpage

{\small \tableofcontents}


\clearpage
\section{Chapter notes}

% ----------------------------------------------------------------------------
\subsection{Introduction}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Probability Distributions}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Linear Models for Regression}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Linear Models for Classification}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Neural networks}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Kernel methods}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sparse Kernel Machines}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Graphical Models}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Mixture Models and EM}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Approximate Interference}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sampling Methods}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Continuous Latent Variables}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sequential Data}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Combining Models}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------





\clearpage

\section{Chapter summaries}

% ----------------------------------------------------------------------------
\subsection{Introduction}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Probability Distributions}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Linear Models for Regression}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Linear Models for Classification}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Neural networks}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Kernel methods}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sparse Kernel Machines}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Graphical Models}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Mixture Models and EM}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Approximate Interference}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sampling Methods}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Continuous Latent Variables}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sequential Data}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Combining Models}
\begin{easylist}[itemize]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
















\clearpage
\section{Exercises}

% ----------------------------------------------------------------------------
\subsection{Introduction}
\subsubsection*{Exercise 1.2}
We start with Equation (1.4) and differentiate it with respect to $w_i$:
\begin{align*}
	\partial_{w_i} \tilde{E}(\vect{w})
	&= \sum_{n=1}^{N} \left[ y(x_n, \vect{w}) - t_n \right] \partial_{w_i} y(x_n, \vect{w}) + \lambda w_i \\
	&= \sum_{n=1}^{N} \left[ y(x_n, \vect{w}) - t_n \right] (x_n)^i + \lambda w_i \\
	&= \sum_{n=1}^{N} \left[ \sum_{j=0}^{M} w_j (x_n)^j - t_n \right] (x_n)^i + \lambda w_i = 0
\end{align*}
Multiplying through the $(x_n)^i$ and rearranging terms yields
\begin{equation*}
	\sum_{j=0}^{M} w_j \underbrace{\sum_{n=1}^{N} (x_n)^{i+j}}_{A_{ij}}
	+
	\lambda w_i = \underbrace{\sum_{n=1}^{N}  (x_n)^i t_n}_{T_{ij}},
\end{equation*}
where the definitions of $A_{ij}$ and $T_{ij}$ are identical to those given in Exercise 1.1.
Finally we employ the kronecker delta symbol $\delta_{ij}$ to pull the $w_i$ into the sum, since $\lambda w_j = \sum_{j=0}^{N} \lambda \delta_{ij} w_i$ we have
\begin{equation*}
	\sum_{j=0}^{M}  \left( A_{ij} + \delta_{ij} \lambda \right) w_j
	 = T_{ij}.
\end{equation*}
This solves the problem.
Notice that in vector notation this system can be written as 
\begin{equation*}
	\left( \vect{\Phi}^T \vect{\Phi} + \vect{I}\lambda \right) \vect{w} = \vect{\Phi}^T \vect{t},
\end{equation*}
where $\vect{\Phi}_{ij}= (x_i)^j$.
Solving the system solves the regularized polynomial fitting problem.


\subsubsection*{Exercise 1.8}
We first show that the expected value is $\mu$.
We define $K(\sigma) = 1 / \sqrt{2 \pi} \sigma$ and evaluate the integral
\begin{align*}
	\E \left[x\right] &= \int K(\sigma) \exp \left( - \frac{1}{2 \sigma^2} \left(x - \mu \right)^2 \right) x \, dx \\
	&= \int K(\sigma) \exp \left( - \frac{1}{2 \sigma^2} z^2 \right) (z + \mu) \, dz \tag{change of variables}\\
	&= \int K(\sigma) \exp \left( - \frac{1}{2 \sigma^2} z^2 \right) z \, dz 
	+
	 \mu \int K(\sigma) \exp \left( - \frac{1}{2 \sigma^2} z^2 \right) \, dz \\
	 & = 0 + \mu,
\end{align*}
where the first integral in the second to last line is zero because it's an odd function integrated over the real line, and the second integral evaluates to $\mu$ since the integrand is unity (it's a centered normal distribution, which has integral $1$).

The second part of the problem asks us to verify that $\E \left[ x^2 \right] = \mu^2 + \sigma^2$.
We factor the normal distribution as $\N\left( x \mid \mu, \sigma^2\right) = K(\sigma^2) E(\sigma^2)$, where
\begin{alignat*}{3}
	& K(\sigma^2) = \left( 2 \pi \sigma^2 \right)^{-1/2} 
	&& \qquad \frac{\partial K}{\partial \sigma^2} = - \left( 2 \pi \sigma^2 \right)^{-3/2} \pi = -K(\sigma^2)^3 \pi \\
	& E(\sigma^2) = \exp \left( - \frac{1}{2 \sigma^2} \left( x - \mu \right)^2 \right)
	&& \qquad \frac{\partial E}{\partial \sigma^2} = \frac{1}{2\sigma^4} \left( x - \mu\right)^2 E(\sigma^2).
\end{alignat*}
This allows us to use the product rule of calculus, we have
\begin{gather*}
	\frac{\partial}{\partial \sigma^2} \left( \int K(\sigma^2) E(\sigma^2) \, dx = 1 \right) \\
	\int K' E + K E' \, dx = 0 \\
	\int KE \left( -\pi K^2 + \frac{1}{2 \sigma^4} (x - \mu)^2 \right) \, dx = 0.
\end{gather*}
Substituting $- \pi K^2 = - 1 / (2 \sigma^2)$, expanding the square term, multiplying out the $KE$ term and performing the integrals, we obtain
\begin{equation*}
	-\frac{1}{2\sigma^2} + \frac{1}{2 \sigma^4} \int KE x^2 \, dx + 0 - \frac{\mu^2}{2 \sigma^4} = 0,
\end{equation*}
and solving this for the unknown integral yields $\operatorname{E}\left[x^2\right] = \int KE x^2 \, dx = \mu^2 + \sigma^2$.

To show that equation (1.51) holds, notice that
\begin{align*}
	\var \left[ x \right] &= \E\left[ (x - \E\left[x\right])^2\right] \\
	&= \E\left[ x^2 - 2 x \mu + \mu ^2\right] \\
	&= \E\left[ x^2\right] - 2 \mu^2 + \mu^2 \\
	&= \E\left[ x^2\right] - \E\left[ x\right],
\end{align*}
where we have used $\mu$ interchangeably with $\E\left[ x\right]$. 



\subsubsection*{Exercise 1.10}
Recall that the definition of the expected value is
$\E\left[x\right] = \int p(x) x \, dx$, and that $\int p(x) \, dx = 1$. 
Statistical independence means that $p(x, y)$ factors as $p(x)p(y)$, so we have
\begin{align*}
	\E\left[x + y\right] &= \iint p(x, y) (x+y) \, dx \, dy \\
	&= \iint p(x) p(y) (x+y) \, dx \, dy \tag{independence} \\
	&= \iint p(x) p(y) x \, dx \, dy + \iint p(x) p(y) y \, dx \, dy \\
	&= \int p(y) \left( \int  p(x)  x \, dx \right) \, dy + \int p(y) y \left( \int p(x)  \, dx \right) \, dy \\
	&= \int p(y) \left( \E \left[ x \right] \right) \, dy + \int p(y) y \left( 1 \right) \, dy \tag{by definition} \\
	&= \E\left[x\right] + \E\left[y\right]
\end{align*}
To show that $\var\left[x + y\right] = \var\left[x\right] + \var\left[y\right]$, we use the preceding result along with the definition from equation (1.38) in \cite{bishop_pattern_2011}  $\var\left[x\right] = \E\left[ (x - \E\left[x\right])^2\right]$ to write
\begin{align*}
	\var\left[x + y\right] &= \E\left[ \left( (x + y) - \E \left[ x + y \right] \right)^2\right] \\
	&= \E\left[ \left( \left(x - \E\left[x\right]\right) + \left(y - \E\left[y\right]\right) \right)^2\right] \tag{rearranging} \\
	&= \E\left[  \left(x - \E\left[x\right]\right)^2 + 2 \left(x - \E\left[x\right]\right) \left(y - \E\left[y\right]\right) + \left(y - \E\left[y\right]\right)^2\right] \\
	&= \E\left[  \left(x - \E\left[x\right]\right)^2 \right]
	+
	\underbrace{\E\left[   2 \left(x - \E\left[x\right]\right) \left(y - \E\left[y\right]\right) \right]}_{0}
	+
	\E\left[   \left(y - \E\left[y\right]\right)^2\right] \\
	&= \var\left[x\right] + \var\left[y\right].
\end{align*}
The cross term vanishes since $x$ and $y$ are independent.
We will not show this in detail, but it can be shown by first noticing that $\E \left[xy\right] = \E \left[x\right] \E \left[y\right]$ when $x$ and $y$ are independent, and then showing that $\E\left[    \left(x - \E\left[x\right]\right) \right] = \E\left[ x \right] - \E\left[ x \right] = 0$.

\subsubsection*{Exercise 1.15}
We split this rather large problem into parts.
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# The redundancy is present due to the fact fact multiplication is commutative, so the weights may be factored out.
	For instance, when $M=2$, we see that 
	\begin{equation*}
		w_{ij}x_i x_j + w_{ji} x_j x_i = (w_{ij} + w_{ji}) x_i x_j = \widetilde{w}_{ij} x_i x_j.
	\end{equation*}
	We reduce redundancy by ordering the products in a common term with $i_1 \geq i_2 \geq \cdots \geq i_M$.
	This ordering corresponds to Equation (1.134) in \cite{bishop_pattern_2011}.
	For instance, instead of summing over terms with $x_1 x_2 x_3$, $x_1 x_3 x_2$, $x_2 x_1 x_3$ and so forth, we make use of a common weight for the $x_3 x_2 x_1$-term.
	
	# The number of terms is the number of terms in the sum nested sum
	\begin{equation*}
		n(D, M) = \sum_{i_1 = 1}^{D} 
		\sum_{i_2 = 1}^{i_1}
		\cdots
		\sum_{i_M = 1}^{i_{M - 1}} 1,
	\end{equation*}
	which contains $M$ sums.
	To prove the recursive formula, we expand the outer sum and notice that the result is $D$ nested sums over $M-1$ sums each.
	We have
	\begin{align*}
		n(D, M) &= \sum_{i_1 = 1}^{D} 
		\sum_{i_2 = 1}^{i_1}
		\cdots
		\sum_{i_M = 1}^{i_{M - 1}} 1 \\
		&= 
		\left( 
		\sum_{i_2 = 1}^{1}
		\cdots
		\sum_{i_M = 1}^{i_{M - 1}} 1
		\right)+\left( 
		\sum_{i_2 = 1}^{2}
		\cdots
		\sum_{i_M = 1}^{i_{M - 1}} 1
		\right)+
		\cdots
		+\left( 
		\sum_{i_2 = 1}^{D}
		\cdots
		\sum_{i_M = 1}^{i_{M - 1}} 1 \right) \\
		&= n(D=1, M-1) + n(D=2, M-1) + \cdots + n(D=D, M-1) \\
		&= \sum_{i=1}^{D} n(i, M-1).
	\end{align*}
	
	# We skip the base case, which is easily verified.
	Assuming the result holds for $D$, we we show that it holds for $D+1$ by writing
	\begin{align*}
		\sum_{i=1}^{D + 1} \frac{(i +M - 2)!}{(i-1)!\, (M-1)!} &=
		\sum_{i=1}^{D} \frac{(i +M - 2)!}{(i-1)! \, (M-1)!}
		+  \frac{(D+M-1)!}{D! \, (M-1)!}.
	\end{align*}
	The sum on the right hand side is the case for $D$, which we assume is true.
	Using this, we write
	\begin{align*}
		\sum_{i=1}^{D + 1} \frac{(i +M - 2)!}{(i-1)! \, (M-1)!} &=
		\frac{(D + M - 1)!}{(D- 1)! \, M!}
		+  \frac{(D+M-1)!}{D! \, (M-1)!} \\
		&=
		\frac{(D + M - 1)! \, D}{D! \, M!}
		+  \frac{(D+M-1)! \, M}{D! \, M!} \\
		&=
		\frac{(D + M - 1)! \, (D + M)}{D! \, M!} = \frac{(D + M)!}{D!\, M!} \\
		&=\frac{((D+1) + M - 1)!}{((D+1) - 1)! \, M!},
	\end{align*}
	which shows that the result holds for $D+1$ when it holds for $D$.
	
	# We skip the base case of the inductive argument, as it should be easy to carry out.
	
	The inductive step is performed as follows.
	Below, the first equality comes from Equation (1.135) in the book,
	the second comes from assuming the result holds for $M-1$,
	and the third comes from (1.136).
	We have that
	\begin{equation*}
		n(D, M) = \sum_{i=1}^{D} n(i, D - 1)
		= \sum_{i=1}^{D} \frac{(i +M - 2)!}{(i-1)!\, (M-1)!} 
		= 
		 \frac{(D + M - 1)!}{(D - 1)! \, M !}.
	\end{equation*}
	Comparing the first and final expression, we observe that if we assume the relation holds for $M-1$, it does indeed hold for $M$ too.
\end{easylist}


\subsubsection*{Exercise 1.21}
Starting with $a \leq b$, we multiply both sides by $a > 0$ to obtain $a^2 \leq ab$.
We can take the square root of both sides and preserve the inequality, since the square root is monotonically increasing.
Doing so, we obtain the desired inequality.

To prove the inequality, we apply the inequality on each term in equation (1.78), then replace the integral over $\mathcal{R}_1 \cup \mathcal{R}_2$ with the real line.
\begin{align*}
	p(\text{mistake}) &= \int_{\mathcal{R}_1} p(\vect{x}, \mathcal{C}_1) \, d \vect{x}
	+
	\int_{\mathcal{R}_2} p(\vect{x}, \mathcal{C}_2) \, d \vect{x} \\
	&\leq 
	 \int_{\mathcal{R}_1} \left\{ p(\vect{x}, \mathcal{C}_1) \, p(\vect{x}, \mathcal{C}_2) \right\}^{1/2} \, d \vect{x}
	+
	\int_{\mathcal{R}_2} \left\{ p(\vect{x}, \mathcal{C}_2) 
	\, p(\vect{x}, \mathcal{C}_1) \right\}^{1/2} \, d \vect{x} \\
	&= \int \left\{ p(\vect{x}, \mathcal{C}_2) \, p(\vect{x}, \mathcal{C}_1) \right\}^{1/2} \, d \vect{x}
\end{align*}

\subsubsection*{Exercise 1.25}
This closely follows the derivation in Section 1.5.5.
If $\vect{x} \in \R^n$ and $\vect{t} \in \R^m$, we view
$\E \left[L (\vect{t}, \vect{y} (\vect{x}))\right]$ as a functional from $\{ f \mid f: \R^n \to \R^m \}$ to $\R$.
Comparing the ordinary derivative and the functional derivative, we see that
\begin{align*}
	f(\vect{x} + \vect{\epsilon}) &= f(\vect{x}) 
	+
	\vect{\epsilon}^T \nabla f(\vect{x}), \quad \text{and}
	\\
	F(\vect{y}(\vect{x}) + \epsilon \vect{\eta}(\vect{x})) &=
	F(\vect{y}(\vect{x})) + \epsilon \int \int  \vect{\eta}^T
	\delta_{\vect{y}_j(\vect{x})} F(\vect{y}(\vect{x}))
	  \, d \vect{t} \, d\vect{x} \\
	&=
	 F(\vect{y}(\vect{x}))
	+
	\epsilon \sum_{j=1}^{n}
	 \int \underbrace{\left( \int \frac{\delta F(\vect{y}(\vect{x}))}{\delta \vect{y}_j(\vect{x})} \, d \vect{t} \right)}_{\text{must be } 0} \eta_j (\vect{x}) \, d \vect{x}.
\end{align*}
The above condition implies that
\begin{equation*}
	\left( \int \frac{\delta F(\vect{y}(\vect{x}))}{\delta \vect{y}_j(\vect{x})} \, d \vect{t} \right)=
	2 \int ( \vect{y}_j(\vect{x}) - \vect{t}_j) p (\vect{x}, \vect{t}) \, d\vect{t}
	= 0,
\end{equation*}
which, following the derivation leading to Equation (1.89), leads to
\begin{equation*}
	\vect{y}_j(\vect{x}) = \int t_j p (\vect{t} \mid \vect{x}) \, d \vect{t} = \E_{\vect{t}} \left[ t_j \mid \vect{x} \right].
\end{equation*}
This applies to any  component $j$ of $\vect{y}(\vect{x})$, so $\vect{y}(\vect{x}) = \E_{\vect{t}} \left[ \vect{t} \mid \vect{x} \right]$.
To show that this reduces to Equation (1.89) in the case of a single target variable, simply define $\vect{t} := (t)$.



\subsubsection*{Exercise 1.33}
A hint as to why this is true is given on page 54, which states that $\H\left[y \mid x\right]$ is ``the average additional information needed to specify $y$.''
If the conditional entropy is zero, then $y$ must be completely specified by $x$, i.e. a function of $x$.
If $\H\left[y \mid x\right] = 0$, then
\begin{equation*}
	\sum_i \sum_j p(x_i, y_j) \ln p (y_j \mid x_i) = 0.
\end{equation*}
Using $ p(x_i, y_j) = p(y_j \mid x_i) p(x_i)$ and rearranging the sums, we obtain
\begin{equation*}
	 \sum_j p(y_j \mid x_i)  \ln p (y_j \mid x_i) \sum_i p(x_i)
	 =
	 \sum_j p(y_j \mid x_i)  \ln p (y_j \mid x_i)
	  = 0.
\end{equation*}
For every $x_i$ with $p(x_i) > 0$, the equation above must be true.
The functional form is $p(z) \ln p(z)$, and this function is negative expect when $z = 0$ and when $z=1$, where it is zero.
Therefore, $p(y_j \mid x_i)  \ln p (y_j \mid x_i)$ must be 0 or 1 for each value of $y_j$. 
However, since $\sum_j p(y_j \mid x_i) = 1$, 


\subsubsection*{Exercise 1.38}
We wish to prove Equation (1.115) based on Equation (1.114).
Clearly the case $M=1$ is true, and we assume the case $M=2$ is true, since this is given by Equation (1.114).
For the recursive step, we assume the identity holds in the base case, and in the $M-1$ case, and then
\begin{align*}
	f \left( \sum_{i=1}^{M} \lambda_i x_i \right)
	&= f \left( \sum_{i=1}^{M-1} \lambda_i x_i + \lambda_M x_M \right)
	= f \left( \frac{\sum_{k=1}^{M-1} \lambda_k}{\sum_{k=1}^{M-1} \lambda_k} \sum_{i=1}^{M-1} \lambda_i x_i + \lambda_M x_M \right).
\end{align*}
The terms $\sum_{k=1}^{M-1} \lambda_k$ and $\lambda_M$ sum to unity, and $\sum_{i=1}^{M-1} \lambda_i x_i / \sum_{k=1}^{M-1} \lambda_k$ is a weighted average of $x$-values.
Since it's a weighted average, it may be treated as just another $x$-value.

Applying the base case for two $x$ values, we obtain
\begin{equation*}
	f \left( \frac{\sum_{k=1}^{M-1} \lambda_k}{\sum_{k=1}^{M-1} \lambda_k} \sum_{i=1}^{M-1} \lambda_i x_i + \lambda_M x_M \right)
	\leq
	\sum_{k=1}^{M-1} \lambda_k f \left( 
	\frac{1}{\sum_{k=1}^{M-1} \lambda_k} \sum_{i=1}^{M-1} \lambda_i x_i
	 \right)
	+
	\lambda_M f \left( x_M \right)
\end{equation*}
Now we can appeal to the $M-1$ case of the inequality, which we assume to be true by the induction hypothesis.
We can appeal to it since the $\lambda_i$ are normalized and sum to unity in the equation above due to the normalizing factor in front of the sum.
To ease notation, we define $\alpha = \sum_{k=1}^{M-1} \lambda_k$ and write
\begin{equation*}
\alpha f \left( 
 \sum_{i=1}^{M-1} \frac{\lambda_i}{\alpha} x_i
\right)
+
\lambda_M f \left( x_M \right)
\leq
\alpha \left( \sum_{i=1}^{M-1} \frac{\lambda_i}{\alpha} f(x_i) \right)
+
\lambda_M f \left( x_M \right)
=
\sum_{i=1}^{M} \lambda_i f(x_i).
\end{equation*}


% ----------------------------------------------------------------------------
\subsection{Probability Distributions}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Linear Models for Regression}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Linear Models for Classification}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Neural networks}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Kernel methods}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sparse Kernel Machines}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Graphical Models}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Mixture Models and EM}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Approximate Interference}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sampling Methods}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Continuous Latent Variables}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Sequential Data}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------
\subsection{Combining Models}
\subsubsection*{Problem X.Y}
\begin{easylist}[enumerate]
	\ListProperties(Space=\listSpace, Space*=\listSpace, Numbers1=l, FinalMark1={)})
	# sdf
\end{easylist}

% ----------------------------------------------------------------------------



































\bibliographystyle{apalike}%alpha, apalike is also good
\bibliography{../bibliography}


\end{document}
