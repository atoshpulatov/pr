
@book{bishop_pattern_2011,
	address = {New York},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {978-0-387-31073-2},
	abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
	language = {English},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	month = apr,
	year = {2006}
}

@book{hastie_elements_2016,
	address = {New York, NY},
	edition = {2nd edition},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	abstract = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
	language = {English},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2016}
}

@book{duda_pattern_2000,
	address = {New York},
	edition = {2 edition},
	title = {Pattern {Classification}},
	isbn = {978-0-471-05669-0},
	abstract = {The first edition, published in 1973, has become a classicreference in the field. Now with the second edition, readers willfind information on key new topics such as neural networks andstatistical pattern recognition, the theory of machine learning,and the theory of invariances. Also included are worked examples,comparisons between different methods, extensive graphics, expandedexercises and computer project topics. An Instructor's Manual presenting detailed solutions to all theproblems in the book is available from the Wiley editorialdepartment.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
	month = nov,
	year = {2000}
}

@article{vanderplas_frequentism_2014,
	title = {Frequentism and {Bayesianism}: {A} {Python}-driven {Primer}},
	shorttitle = {Frequentism and {Bayesianism}},
	url = {https://arxiv.org/abs/1411.5018v1},
	abstract = {This paper presents a brief, semi-technical comparison of the essential
features of the frequentist and Bayesian approaches to statistical inference,
with several illustrative examples implemented in Python. The differences
between frequentism and Bayesianism fundamentally stem from differing
definitions of probability, a philosophical divide which leads to distinct
approaches to the solution of statistical problems as well as contrasting ways
of asking and answering questions about unknown parameters. After an
example-driven discussion of these differences, we briefly compare several
leading Python statistical packages which implement frequentist inference using
classical methods and Bayesian inference using Markov Chain Monte Carlo.},
	language = {en},
	urldate = {2018-12-29},
	author = {VanderPlas, Jake},
	month = nov,
	year = {2014},
	file = {Full Text PDF:/home/tommy/Zotero/storage/6VGZYELC/VanderPlas - 2014 - Frequentism and Bayesianism A Python-driven Prime.pdf:application/pdf;Snapshot:/home/tommy/Zotero/storage/PZPX2LZ8/1411.html:text/html}
}
